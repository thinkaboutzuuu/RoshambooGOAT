{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "odj1Coq5H080"
      },
      "outputs": [],
      "source": [
        "#@title ##### License { display-mode: \"form\" }\n",
        "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOOzDGYAZcW3"
      },
      "source": [
        "# OpenSpiel: RRPS Example\n",
        "\n",
        "* This Colab gets you started with installing OpenSpiel and its dependencies.\n",
        "* OpenSpiel is a framework for reinforcement learning in games.\n",
        "* For a longer intro to OpenSpiel, see [the tutorial video](https://www.youtube.com/watch?v=8NCPqtPwlFQ), [documentation](https://openspiel.readthedocs.io/en/latest/), or [API reference](https://openspiel.readthedocs.io/en/latest/api_reference.html).\n",
        "* This colab also includes examples of how to get started with the Roshambo/RRPS environment and bots. It is based on [roshambo_population_example.py](https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/examples/roshambo_population_example.py)\n",
        "* For more info on Roshambo, see the [RRPS benchmark paper](https://openreview.net/pdf?id=gQnJ7ODIAx)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC6kQBzWahEF"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2_Vbijh4FlZ"
      },
      "source": [
        "Install OpenSpiel via pip:\n",
        "\n",
        "Note that if you are not using a colab, then you would use\n",
        "python3 -m pip install open_spiel\n",
        "Additional information about installing OpenSpiel can be found at the links above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQc12Xrn4CXU",
        "outputId": "d1557aa7-fd7c-43ed-a89e-81f44f5b8222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: open_spiel in /opt/anaconda3/envs/emg/lib/python3.14/site-packages (1.6.9)\n",
            "Requirement already satisfied: pip>=20.0.2 in /Users/guanyulu/.local/lib/python3.14/site-packages (from open_spiel) (25.3)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /Users/guanyulu/.local/lib/python3.14/site-packages (from open_spiel) (25.4.0)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /Users/guanyulu/.local/lib/python3.14/site-packages (from open_spiel) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.21.5 in /Users/guanyulu/.local/lib/python3.14/site-packages (from open_spiel) (2.3.5)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /opt/anaconda3/envs/emg/lib/python3.14/site-packages (from open_spiel) (1.16.3)\n",
            "Requirement already satisfied: ml-collections>=0.1.1 in /Users/guanyulu/.local/lib/python3.14/site-packages (from open_spiel) (1.1.0)\n",
            "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/emg/lib/python3.14/site-packages (from ml-collections>=0.1.1->open_spiel) (6.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade open_spiel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUtlXZ8FBnAL"
      },
      "source": [
        "# Simple example of OpenSpiel API: uniform random trajectory on Tic-Tac-Toe\n",
        "\n",
        "This example is not used for RRPS, but it shows how to load a game in OpenSpiel, how to access a game state, and implements a random strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewMXCaUw8d9Q",
        "outputId": "8ee7a18f-f2e0-4d21-e7ca-f7c405428c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...\n",
            "...\n",
            "x..\n",
            "\n",
            "...\n",
            "o..\n",
            "x..\n",
            "\n",
            "...\n",
            "o..\n",
            "xx.\n",
            "\n",
            ".o.\n",
            "o..\n",
            "xx.\n",
            "\n",
            ".o.\n",
            "o.x\n",
            "xx.\n",
            "\n",
            ".o.\n",
            "oox\n",
            "xx.\n",
            "\n",
            "xo.\n",
            "oox\n",
            "xx.\n",
            "\n",
            "xoo\n",
            "oox\n",
            "xx.\n",
            "\n",
            "xoo\n",
            "oox\n",
            "xxx\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pyspiel\n",
        "\n",
        "game = pyspiel.load_game(\"tic_tac_toe\")\n",
        "state = game.new_initial_state()\n",
        "\n",
        "while not state.is_terminal():\n",
        "  state.apply_action(np.random.choice(state.legal_actions()))\n",
        "  print(str(state) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2hLkPIHLjjV"
      },
      "source": [
        "# Getting started with RRPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iF2eZXwmMJsa"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "\n",
        "from open_spiel.python import rl_agent\n",
        "from open_spiel.python import rl_environment\n",
        "import pyspiel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4SavmNn1MG-N"
      },
      "outputs": [],
      "source": [
        "# Some helper classes and functions.\n",
        "# DO NOT CHANGE.\n",
        "\n",
        "class BotAgent(rl_agent.AbstractAgent):\n",
        "  \"\"\"Agent class that wraps a bot.\n",
        "\n",
        "  Note, the environment must include the OpenSpiel state in its observations,\n",
        "  which means it must have been created with use_full_state=True.\n",
        "\n",
        "  This is a simple wrapper that lets the RPS bots be interpreted as agents under\n",
        "  the RL API.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_actions, bot, name=\"bot_agent\"):\n",
        "    assert num_actions > 0\n",
        "    self._bot = bot\n",
        "    self._num_actions = num_actions\n",
        "\n",
        "  def restart(self):\n",
        "    self._bot.restart()\n",
        "\n",
        "  def step(self, time_step, is_evaluation=False):\n",
        "    # If it is the end of the episode, don't select an action.\n",
        "    if time_step.last():\n",
        "      return\n",
        "    _, state = pyspiel.deserialize_game_and_state(\n",
        "        time_step.observations[\"serialized_state\"])\n",
        "    action = self._bot.step(state)\n",
        "    probs = np.zeros(self._num_actions)\n",
        "    probs[action] = 1.0\n",
        "    return rl_agent.StepOutput(action=action, probs=probs)\n",
        "\n",
        "\n",
        "#  We will use this function to evaluate the agents. Do not change.\n",
        "\n",
        "def eval_agents(env, agents, num_players, num_episodes, verbose=False):\n",
        "  \"\"\"Evaluate the agent.\n",
        "\n",
        "  Runs a number of episodes and returns the average returns for each agent as\n",
        "  a numpy array.\n",
        "\n",
        "  Arguments:\n",
        "    env: the RL environment,\n",
        "    agents: a list of agents (size 2),\n",
        "    num_players: number of players in the game (for RRPS, this is 2),\n",
        "    num_episodes: number of evaluation episodes to run.\n",
        "    verbose: whether to print updates after each episode.\n",
        "  \"\"\"\n",
        "  sum_episode_rewards = np.zeros(num_players)\n",
        "  for ep in range(num_episodes):\n",
        "    for agent in agents:\n",
        "      # Bots need to be restarted at the start of the episode.\n",
        "      if hasattr(agent, \"restart\"):\n",
        "        agent.restart()\n",
        "    time_step = env.reset()\n",
        "    episode_rewards = np.zeros(num_players)\n",
        "    while not time_step.last():\n",
        "      agents_output = [\n",
        "          agent.step(time_step, is_evaluation=True) for agent in agents\n",
        "      ]\n",
        "      action_list = [agent_output.action for agent_output in agents_output]\n",
        "      time_step = env.step(action_list)\n",
        "      episode_rewards += time_step.rewards\n",
        "    sum_episode_rewards += episode_rewards\n",
        "    if verbose:\n",
        "      print(f\"Finished episode {ep}, \"\n",
        "            + f\"avg returns: {sum_episode_rewards / (ep+1)}\")\n",
        "\n",
        "  return sum_episode_rewards / num_episodes\n",
        "\n",
        "\n",
        "def print_roshambo_bot_names_and_ids(roshambo_bot_names):\n",
        "  print(\"Roshambo bot population:\")\n",
        "  for i in range(len(roshambo_bot_names)):\n",
        "    print(f\"{i}: {roshambo_bot_names[i]}\")\n",
        "\n",
        "def create_roshambo_bot_agent(player_id, num_actions, bot_names, pop_id):\n",
        "  name = bot_names[pop_id]\n",
        "  # Creates an OpenSpiel bot with the default number of throws\n",
        "  # (pyspiel.ROSHAMBO_NUM_THROWS). To create one for a different number of\n",
        "  # throws per episode, add the number as the third argument here.\n",
        "  bot = pyspiel.make_roshambo_bot(player_id, name)\n",
        "  return BotAgent(num_actions, bot, name=name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FZlLow5w5da"
      },
      "source": [
        "#The following functions are used to load the bots from the original RRPS competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zimu2jMiNuYf",
        "outputId": "beed88d0-477f-432d-e867-76fa2a8b0a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading bot population...\n",
            "Population size: 43\n",
            "Roshambo bot population:\n",
            "0: actr_lag2_decay\n",
            "1: adddriftbot2\n",
            "2: addshiftbot3\n",
            "3: antiflatbot\n",
            "4: antirotnbot\n",
            "5: biopic\n",
            "6: boom\n",
            "7: copybot\n",
            "8: debruijn81\n",
            "9: driftbot\n",
            "10: flatbot3\n",
            "11: foxtrotbot\n",
            "12: freqbot2\n",
            "13: granite\n",
            "14: greenberg\n",
            "15: halbot\n",
            "16: inocencio\n",
            "17: iocainebot\n",
            "18: marble\n",
            "19: markov5\n",
            "20: markovbails\n",
            "21: mixed_strategy\n",
            "22: mod1bot\n",
            "23: multibot\n",
            "24: peterbot\n",
            "25: phasenbott\n",
            "26: pibot\n",
            "27: piedra\n",
            "28: predbot\n",
            "29: r226bot\n",
            "30: randbot\n",
            "31: robertot\n",
            "32: rockbot\n",
            "33: rotatebot\n",
            "34: russrocker4\n",
            "35: shofar\n",
            "36: sunCrazybot\n",
            "37: sunNervebot\n",
            "38: sweetrock\n",
            "39: switchalot\n",
            "40: switchbot\n",
            "41: textbot\n",
            "42: zq_move\n"
          ]
        }
      ],
      "source": [
        "# Some basic info and initialize the population\n",
        "\n",
        "# print(pyspiel.ROSHAMBO_NUM_BOTS)    # 43 bots\n",
        "# print(pyspiel.ROSHAMBO_NUM_THROWS)  # 1000 steps per episode\n",
        "\n",
        "# The recall is how many of the most recent actions are presented to the RL\n",
        "# agents as part of their observations. Note: this is just for the RL agents\n",
        "# like DQN etc... every bot has access to the full history.\n",
        "RECALL = 20\n",
        "\n",
        "# The population of 43 bots. See the RRPS paper for high-level descriptions of\n",
        "# what each bot does.\n",
        "\n",
        "print(\"Loading bot population...\")\n",
        "pop_size = pyspiel.ROSHAMBO_NUM_BOTS\n",
        "print(f\"Population size: {pop_size}\")\n",
        "roshambo_bot_names = pyspiel.roshambo_bot_names()\n",
        "roshambo_bot_names.sort()\n",
        "print_roshambo_bot_names_and_ids(roshambo_bot_names)\n",
        "\n",
        "bot_id = 0\n",
        "roshambo_bot_ids = {}\n",
        "for name in roshambo_bot_names:\n",
        "  roshambo_bot_ids[name] = bot_id\n",
        "  bot_id += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_qAMW6p5btS"
      },
      "source": [
        "#Example showing how to load to agents from the RRPS bot population and evalute them against each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-u0LlkiOCD6",
        "outputId": "8b85d19d-21f7-4291-90b6-10796d883104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run.\n",
            "Finished episode 0, avg returns: [ 1. -1.]\n",
            "Finished episode 1, avg returns: [ 5.5 -5.5]\n",
            "Finished episode 2, avg returns: [ 4.66666667 -4.66666667]\n",
            "Finished episode 3, avg returns: [ 13.25 -13.25]\n",
            "Finished episode 4, avg returns: [ 14. -14.]\n",
            "Finished episode 5, avg returns: [ 27.16666667 -27.16666667]\n",
            "Finished episode 6, avg returns: [ 20.71428571 -20.71428571]\n",
            "Finished episode 7, avg returns: [ 19.625 -19.625]\n",
            "Finished episode 8, avg returns: [ 18.55555556 -18.55555556]\n",
            "Finished episode 9, avg returns: [ 19.9 -19.9]\n",
            "Avg return  [ 19.9 -19.9]\n"
          ]
        }
      ],
      "source": [
        "# Example: create an RL environment, and two agents from the bot population and\n",
        "# evaluate these two agents head-to-head.\n",
        "\n",
        "# Note that the include_full_state variable has to be enabled because the\n",
        "# BotAgent needs access to the full state.\n",
        "env = rl_environment.Environment(\n",
        "    \"repeated_game(stage_game=matrix_rps(),num_repetitions=\" +\n",
        "    f\"{pyspiel.ROSHAMBO_NUM_THROWS},\" +\n",
        "    f\"recall={RECALL})\",\n",
        "    include_full_state=True)\n",
        "num_players = 2\n",
        "num_actions = env.action_spec()[\"num_actions\"]\n",
        "# Learning agents might need this:\n",
        "# info_state_size = env.observation_spec()[\"info_state\"][0]\n",
        "\n",
        "# Create two bot agents\n",
        "p0_pop_id = 0   # actr_lag2_decay\n",
        "p1_pop_id = 1   # adddriftbot2\n",
        "agents = [\n",
        "    create_roshambo_bot_agent(0, num_actions, roshambo_bot_names, p0_pop_id),\n",
        "    create_roshambo_bot_agent(1, num_actions, roshambo_bot_names, p1_pop_id)\n",
        "]\n",
        "\n",
        "print(\"Starting eval run.\")\n",
        "avg_eval_returns = eval_agents(env, agents, num_players, 10, verbose=True)\n",
        "\n",
        "print(\"Avg return \", avg_eval_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKedmD2G3Axh"
      },
      "source": [
        "#Basic Template for an RL agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpbyfKe2SV1s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class RPSLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size=3, emb_dim=8, hidden_size=64):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_size, num_layers=1, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 3)  # logits for R,P,S\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x: (batch, T) of ints 0/1/2\n",
        "        emb = self.embed(x)\n",
        "        out, hidden = self.lstm(emb, hidden)  # out: (batch, T, hidden)\n",
        "        last = out[:, -1, :]\n",
        "        logits = self.fc(last)                 # (batch, T, 3)\n",
        "        return logits, hidden\n",
        "    \n",
        "\n",
        "class MyAgent(rl_agent.AbstractAgent):\n",
        "    \"\"\"\n",
        "    Greenberg-lite ensemble agent for repeated RPS:\n",
        "\n",
        "    - Maintains several simple \"expert\" strategies (predictors + best-response).\n",
        "    - Tracks a score for each expert based on how well it *would have* done.\n",
        "    - At each step, picks the action of the best-scoring expert (with some\n",
        "      epsilon exploration).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 player_id,\n",
        "                 num_actions=3,\n",
        "                 score_alpha=0.5,      \n",
        "                 epsilon_action=0.1, \n",
        "                 epsilon_expert=0.05, \n",
        "                 name=\"ensemble_agent\",\n",
        "                 lstm_model_path=\"rps_lstm.pt\",\n",
        "                 lstm_seq_len=200, \n",
        "                 gamma = 0.95,\n",
        "                 mdp_interval = 50):\n",
        "        super().__init__(player_id=player_id, name=name)\n",
        "        assert num_actions == 3, \"RRPS uses 3 actions (R,P,S).\"\n",
        "        self._player_id = player_id\n",
        "        self._num_actions = num_actions\n",
        "\n",
        "        self._score_alpha = score_alpha\n",
        "        self._eps_action = epsilon_action\n",
        "        self._eps_expert = epsilon_expert\n",
        "\n",
        "        self._payoff = np.array([\n",
        "            [ 0, -1,  1],  \n",
        "            [ 1,  0, -1],  \n",
        "            [-1,  1,  0],  \n",
        "        ], dtype=float)\n",
        "\n",
        "        # Number of experts we'll define\n",
        "        self._num_experts = 7\n",
        "        self._lstm_seq_len = lstm_seq_len\n",
        "        self._device = torch.device(\"mps\")\n",
        "        self._lstm_model = RPSLSTM(vocab_size=4, emb_dim=16, hidden_size=64).to(self._device)\n",
        "        state_dict = torch.load(lstm_model_path, map_location=self._device)\n",
        "        self._lstm_model.load_state_dict(state_dict)\n",
        "        self._lstm_model.eval()\n",
        "        self._lstm_hidden = None\n",
        "\n",
        "        self._mdp_num_states = self._num_actions*self._num_actions + 1\n",
        "        self._mdp_start_state = self._mdp_num_states-1\n",
        "        self._mdp_gamma = gamma\n",
        "        self._mdp_plan_interval = mdp_interval\n",
        "\n",
        "        self._mdp_N_sa = np.zeros((self._mdp_num_states, self._num_actions), dtype=np.float64)\n",
        "        self._mdp_R_sa_sum = np.zeros((self._mdp_num_states, self._num_actions), dtype=np.float64)\n",
        "        self._mdp_N_sas = np.zeros(\n",
        "            (self._mdp_num_states, self._num_actions, self._mdp_num_states), dtype=np.float64\n",
        "        )\n",
        "\n",
        "        # Value function and policy\n",
        "        self._mdp_V = np.zeros(self._mdp_num_states, dtype=np.float64)\n",
        "        self._mdp_policy = -np.ones(self._mdp_num_states, dtype=int)\n",
        "\n",
        "        self._mdp_prev_state = None\n",
        "        self._mdp_prev_action = None\n",
        "        self._mdp_steps_since_plan = 0\n",
        "\n",
        "        self.restart()\n",
        "\n",
        "    # ------------ Episode reset ------------\n",
        "\n",
        "    def restart(self):\n",
        "        # Opponent statistics\n",
        "        self._opp_counts = np.ones(self._num_actions, dtype=float)  # smoothed counts\n",
        "        self._trans_counts = np.ones((self._num_actions, self._num_actions),\n",
        "                                     dtype=float)  # P(opp_t | opp_{t-1})\n",
        "\n",
        "        self._last_opp_action = None\n",
        "        self._last_my_action = None\n",
        "\n",
        "        # Expert scores and last actions they recommended\n",
        "        self._expert_scores = np.zeros(self._num_experts, dtype=float)\n",
        "        self._last_expert_actions = [None] * self._num_experts\n",
        "\n",
        "        # Track how much of history we've processed (if we use it later)\n",
        "        self._last_history_len = 0\n",
        "\n",
        "        self._opp_hist = []\n",
        "        self._lstm_hidden = None\n",
        "\n",
        "        self._mdp_prev_state = None\n",
        "        self._mdp_prev_action = None\n",
        "        self._mdp_steps_since_plan = 0\n",
        "\n",
        "    # ------------ Utility helpers ------------\n",
        "\n",
        "    def _beat(self, move):\n",
        "        #Return the action that beats 'move'\n",
        "        if move is None:\n",
        "            return np.random.randint(self._num_actions)\n",
        "        return (move + 1) % self._num_actions\n",
        "\n",
        "    def _update_opp_stats(self, state):\n",
        "        #Update opponent statistics based on game history.\n",
        "        history = state.history()\n",
        "        if len(history) == 0:\n",
        "            return None\n",
        "\n",
        "        action = history[-1]\n",
        "        # Update counts\n",
        "        self._opp_counts[action] += 1\n",
        "        if self._last_opp_action is not None:\n",
        "            self._trans_counts[self._last_opp_action, action] += 1\n",
        "        self._last_opp_action = action\n",
        "        self._opp_hist.append(action)\n",
        "        return action\n",
        "    \n",
        "    def _lstm_predict_opp(self):\n",
        "        if len(self._opp_hist) == 0:\n",
        "            return None\n",
        "\n",
        "        # take last seq_len moves (pad/truncate)\n",
        "        seq = self._opp_hist[-self._lstm_seq_len:]\n",
        "        seq_tensor = torch.tensor(seq, dtype=torch.long, device=self._device)\n",
        "        seq_tensor = seq_tensor.unsqueeze(0)  # shape (1, T)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # assume model returns (logits, hidden)\n",
        "            logits, self._lstm_hidden = self._lstm_model(seq_tensor, self._lstm_hidden)\n",
        "            # use last time step logits\n",
        "            # last_logits = logits[:, -1, :]      # shape (1, 3)\n",
        "            last_logits = logits      # shape (1, 3)\n",
        "\n",
        "            probs = F.softmax(last_logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "        pred = int(np.argmax(probs))\n",
        "        return pred\n",
        "    \n",
        "    def _mdp_current_state(self):\n",
        "        \"\"\"Encode current MDP state from last joint action.\"\"\"\n",
        "        if self._last_my_action is None or self._last_opp_action is None:\n",
        "            return self._mdp_start_state\n",
        "        return self._last_my_action * self._num_actions + self._last_opp_action\n",
        "\n",
        "    def _mdp_observe_transition(self, reward, curr_state):\n",
        "        \"\"\"Update empirical MDP using previous (s,a) and current state.\"\"\"\n",
        "        if self._mdp_prev_state is None or self._mdp_prev_action is None:\n",
        "            return\n",
        "        s = self._mdp_prev_state\n",
        "        a = self._mdp_prev_action\n",
        "        s_next = curr_state\n",
        "\n",
        "        self._mdp_N_sa[s, a] += 1.0\n",
        "        self._mdp_R_sa_sum[s, a] += reward\n",
        "        self._mdp_N_sas[s, a, s_next] += 1.0\n",
        "\n",
        "        self._mdp_steps_since_plan += 1\n",
        "        if self._mdp_steps_since_plan >= self._mdp_plan_interval:\n",
        "            self._mdp_plan_policy()\n",
        "            self._mdp_steps_since_plan = 0\n",
        "\n",
        "    def _mdp_plan_policy(self, max_iters=100, tol=1e-4):\n",
        "        \"\"\"Value iteration on empirical MDP; update V and π(s).\"\"\"\n",
        "        V = self._mdp_V.copy()\n",
        "        S = self._mdp_num_states\n",
        "        A = self._num_actions\n",
        "        gamma = self._mdp_gamma\n",
        "\n",
        "        for _ in range(max_iters):\n",
        "            delta = 0.0\n",
        "            for s in range(S):\n",
        "                best_val = None\n",
        "                for a in range(A):\n",
        "                    n_sa = self._mdp_N_sa[s, a]\n",
        "                    if n_sa <= 0:\n",
        "                        continue\n",
        "                    r_hat = self._mdp_R_sa_sum[s, a] / n_sa\n",
        "                    p_hat = self._mdp_N_sas[s, a] / n_sa  # shape (S,)\n",
        "                    val = r_hat + gamma * np.dot(p_hat, V)\n",
        "                    if (best_val is None) or (val > best_val):\n",
        "                        best_val = val\n",
        "                if best_val is None:\n",
        "                    best_val = 0.0\n",
        "                delta = max(delta, abs(best_val - V[s]))\n",
        "                V[s] = best_val\n",
        "            if delta < tol:\n",
        "                break\n",
        "\n",
        "        # Extract greedy policy\n",
        "        policy = -np.ones(S, dtype=int)\n",
        "        for s in range(S):\n",
        "            best_val = None\n",
        "            best_a = 0\n",
        "            for a in range(A):\n",
        "                n_sa = self._mdp_N_sa[s, a]\n",
        "                if n_sa <= 0:\n",
        "                    continue\n",
        "                r_hat = self._mdp_R_sa_sum[s, a] / n_sa\n",
        "                p_hat = self._mdp_N_sas[s, a] / n_sa\n",
        "                val = r_hat + gamma * np.dot(p_hat, V)\n",
        "                if (best_val is None) or (val > best_val):\n",
        "                    best_val = val\n",
        "                    best_a = a\n",
        "            if best_val is not None:\n",
        "                policy[s] = best_a\n",
        "\n",
        "        self._mdp_V = V\n",
        "        self._mdp_policy = policy\n",
        "\n",
        "    # ------------ Expert strategies ------------\n",
        "\n",
        "    def _expert_actions(self):\n",
        "        #Compute each expert's recommended action for step.\n",
        "        actions = []\n",
        "\n",
        "        # Expert 0: Frequency-based best response\n",
        "        # Predict opp's most frequent move overall.\n",
        "        opp_probs = self._opp_counts / np.sum(self._opp_counts)\n",
        "        pred0 = int(np.argmax(opp_probs))\n",
        "        actions.append(self._beat(pred0))\n",
        "\n",
        "        # Expert 1: Last-opponent-move best response\n",
        "        # Predict they repeat their last move.\n",
        "        actions.append(self._beat(self._last_opp_action))\n",
        "\n",
        "        # Expert 2: Markov(1) best response\n",
        "        # Predict based on last opp move -> next opp move.\n",
        "        if self._last_opp_action is not None:\n",
        "            row = self._trans_counts[self._last_opp_action]\n",
        "            pred2 = int(np.argmax(row))\n",
        "            actions.append(self._beat(pred2))\n",
        "        else:\n",
        "            # fallback to frequency BR\n",
        "            actions.append(self._beat(pred0))\n",
        "\n",
        "        # Expert 3: Mirror-me assumption\n",
        "        # Predict opp plays what I played last time.\n",
        "        actions.append(self._beat(self._last_my_action))\n",
        "\n",
        "        # Expert 4: ML (LSTM)\n",
        "        pred_lstm = self._lstm_predict_opp()\n",
        "        if pred_lstm is not None:\n",
        "            actions.append(self._beat(pred_lstm))\n",
        "        else:\n",
        "            # fallback: behave like frequency BR\n",
        "            actions.append(self._beat(pred0))\n",
        "\n",
        "        # Expert 5: random bullshit\n",
        "        import random\n",
        "        actions.append(random.randint(0, 2))\n",
        "\n",
        "        # Expert 6: MDP expert (greedy from π(s))\n",
        "        curr_state = self._mdp_current_state()\n",
        "        if 0 <= curr_state < self._mdp_num_states and self._mdp_policy[curr_state] != -1:\n",
        "            mdp_action = int(self._mdp_policy[curr_state])\n",
        "        else:\n",
        "            # fallback: same as freq-BR\n",
        "            mdp_action = self._beat(pred0)\n",
        "        actions.append(mdp_action)\n",
        "\n",
        "        return actions\n",
        "\n",
        "    # ------------ Expert scoring ------------\n",
        "\n",
        "    def _update_expert_scores(self, last_opp_action):\n",
        "        \"\"\"\n",
        "        Update each expert's score based on what happened in the *previous* round.\n",
        "\n",
        "        We use the payoff matrix: if expert i had played its suggested action,\n",
        "        what reward would it have gotten vs last_opp_action?\n",
        "        \"\"\"\n",
        "        if last_opp_action is None:\n",
        "            return  # nothing to update on the first move\n",
        "\n",
        "        for i in range(self._num_experts):\n",
        "            a_i = self._last_expert_actions[i]\n",
        "            if a_i is None:\n",
        "                continue\n",
        "            payoff_i = self._payoff[a_i, last_opp_action]\n",
        "            # Exponential moving average of payoff\n",
        "            self._expert_scores[i] = (\n",
        "                (1 - self._score_alpha) * self._expert_scores[i]\n",
        "                + self._score_alpha * payoff_i\n",
        "            )\n",
        "\n",
        "    # ------------ Main step ------------\n",
        "\n",
        "    def step(self, time_step, is_evaluation=False):\n",
        "        # Terminal: return dummy\n",
        "        if time_step.last():\n",
        "            probs = np.ones(self._num_actions) / self._num_actions\n",
        "            return rl_agent.StepOutput(action=0, probs=probs)\n",
        "\n",
        "        # Deserialize game state\n",
        "        game, state = pyspiel.deserialize_game_and_state(\n",
        "            time_step.observations[\"serialized_state\"])\n",
        "\n",
        "        # 1) Update opponent stats (includes last_opp_action)\n",
        "        last_opp_action = self._update_opp_stats(state)\n",
        "\n",
        "        curr_mdp_state = self._mdp_current_state()\n",
        "        reward = None\n",
        "        if time_step.rewards is not None:\n",
        "            reward = time_step.rewards[self._player_id]\n",
        "        elif self._last_my_action is not None and last_opp_action is not None:\n",
        "            reward = self._payoff[self._last_my_action, last_opp_action]\n",
        "        if reward is not None:\n",
        "            self._mdp_observe_transition(reward, curr_mdp_state)\n",
        "\n",
        "        # 2) Update expert scores based on previous round outcome\n",
        "        # We don't need time_step.rewards here; we recompute payoff from matrix.\n",
        "        self._update_expert_scores(last_opp_action)\n",
        "\n",
        "        # 3) Each expert proposes an action for THIS round\n",
        "        expert_actions = self._expert_actions()\n",
        "        self._last_expert_actions = list(expert_actions)  # store for next scoring\n",
        "\n",
        "        # 4) Choose which expert to follow (epsilon-greedy over expert scores)\n",
        "        if np.random.rand() < self._eps_expert:\n",
        "            chosen_expert = np.random.randint(self._num_experts)\n",
        "        else:\n",
        "            chosen_expert = int(np.argmax(self._expert_scores))\n",
        "\n",
        "        chosen_action = expert_actions[chosen_expert]\n",
        "\n",
        "        # 5) Add action-level randomness for robustness\n",
        "        if np.random.rand() < self._eps_action:\n",
        "            action = np.random.randint(self._num_actions)\n",
        "        else:\n",
        "            action = chosen_action\n",
        "\n",
        "        # Save my last action for next round's mirror expert\n",
        "        self._last_my_action = action\n",
        "\n",
        "        self._mdp_prev_state = curr_mdp_state\n",
        "        self._mdp_prev_action = action\n",
        "\n",
        "        # 6) Build probability distribution (mostly on chosen_action)\n",
        "        probs = np.ones(self._num_actions) * (self._eps_action / self._num_actions)\n",
        "        probs[chosen_action] += 1.0 - self._eps_action\n",
        "\n",
        "        return rl_agent.StepOutput(action=action, probs=probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtjPe7O4as2V",
        "outputId": "90e6e6a6-280e-4626-ff4a-66dc8d185cda"
      },
      "outputs": [],
      "source": [
        "# # Just trying an example out.\n",
        "\n",
        "my_agent = MyAgent(player_id=0, name=\"kate_agent\", epsilon_action=0.1)\n",
        "print(my_agent._num_actions)\n",
        "\n",
        "\n",
        "# p1_pop_id = 17  # adddriftbot2\n",
        "# agents = [\n",
        "#     my_agent,\n",
        "#     create_roshambo_bot_agent(1, num_actions, roshambo_bot_names, p1_pop_id)\n",
        "# ]\n",
        "\n",
        "\n",
        "# print(\"Starting eval run.\")\n",
        "# avg_eval_returns = eval_agents(env, agents, num_players, 10, verbose=True)\n",
        "\n",
        "# print(\"Avg return \", avg_eval_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating against bot 0: actr_lag2_decay\n",
            "→ My agent vs actr_lag2_decay: 6.4000\n",
            "\n",
            "Evaluating against bot 1: adddriftbot2\n",
            "→ My agent vs adddriftbot2: 8.4000\n",
            "\n",
            "Evaluating against bot 2: addshiftbot3\n",
            "→ My agent vs addshiftbot3: 36.2000\n",
            "\n",
            "Evaluating against bot 3: antiflatbot\n",
            "→ My agent vs antiflatbot: 190.5000\n",
            "\n",
            "Evaluating against bot 4: antirotnbot\n",
            "→ My agent vs antirotnbot: 34.7000\n",
            "\n",
            "Evaluating against bot 5: biopic\n",
            "→ My agent vs biopic: 21.5000\n",
            "\n",
            "Evaluating against bot 6: boom\n",
            "→ My agent vs boom: 13.1000\n",
            "\n",
            "Evaluating against bot 7: copybot\n",
            "→ My agent vs copybot: 182.0000\n",
            "\n",
            "Evaluating against bot 8: debruijn81\n",
            "→ My agent vs debruijn81: 29.7000\n",
            "\n",
            "Evaluating against bot 9: driftbot\n",
            "→ My agent vs driftbot: 11.3000\n",
            "\n",
            "Evaluating against bot 10: flatbot3\n",
            "→ My agent vs flatbot3: 26.4000\n",
            "\n",
            "Evaluating against bot 11: foxtrotbot\n",
            "→ My agent vs foxtrotbot: 17.3000\n",
            "\n",
            "Evaluating against bot 12: freqbot2\n",
            "→ My agent vs freqbot2: 157.9000\n",
            "\n",
            "Evaluating against bot 13: granite\n",
            "→ My agent vs granite: 31.4000\n",
            "\n",
            "Evaluating against bot 14: greenberg\n",
            "→ My agent vs greenberg: 2.9000\n",
            "\n",
            "Evaluating against bot 15: halbot\n",
            "→ My agent vs halbot: 8.9000\n",
            "\n",
            "Evaluating against bot 16: inocencio\n",
            "→ My agent vs inocencio: 28.2000\n",
            "\n",
            "Evaluating against bot 17: iocainebot\n",
            "→ My agent vs iocainebot: 3.3000\n",
            "\n",
            "Evaluating against bot 18: marble\n",
            "→ My agent vs marble: 18.1000\n",
            "\n",
            "Evaluating against bot 19: markov5\n",
            "→ My agent vs markov5: 2.1000\n",
            "\n",
            "Evaluating against bot 20: markovbails\n",
            "→ My agent vs markovbails: 13.3000\n",
            "\n",
            "Evaluating against bot 21: mixed_strategy\n",
            "→ My agent vs mixed_strategy: 38.9000\n",
            "\n",
            "Evaluating against bot 22: mod1bot\n",
            "→ My agent vs mod1bot: 5.8000\n",
            "\n",
            "Evaluating against bot 23: multibot\n",
            "→ My agent vs multibot: 72.1000\n",
            "\n",
            "Evaluating against bot 24: peterbot\n",
            "→ My agent vs peterbot: 17.6000\n",
            "\n",
            "Evaluating against bot 25: phasenbott\n",
            "→ My agent vs phasenbott: -1.7000\n",
            "\n",
            "Evaluating against bot 26: pibot\n",
            "→ My agent vs pibot: 44.9000\n",
            "\n",
            "Evaluating against bot 27: piedra\n",
            "→ My agent vs piedra: 29.3000\n",
            "\n",
            "Evaluating against bot 28: predbot\n",
            "→ My agent vs predbot: 18.4000\n",
            "\n",
            "Evaluating against bot 29: r226bot\n",
            "→ My agent vs r226bot: 38.7000\n",
            "\n",
            "Evaluating against bot 30: randbot\n",
            "→ My agent vs randbot: -1.9000\n",
            "\n",
            "Evaluating against bot 31: robertot\n",
            "→ My agent vs robertot: 5.0000\n",
            "\n",
            "Evaluating against bot 32: rockbot\n",
            "→ My agent vs rockbot: 199.8000\n",
            "\n",
            "Evaluating against bot 33: rotatebot\n",
            "→ My agent vs rotatebot: 203.8000\n",
            "\n",
            "Evaluating against bot 34: russrocker4\n",
            "→ My agent vs russrocker4: -11.0000\n",
            "\n",
            "Evaluating against bot 35: shofar\n",
            "→ My agent vs shofar: 18.3000\n",
            "\n",
            "Evaluating against bot 36: sunCrazybot\n",
            "→ My agent vs sunCrazybot: 111.5000\n",
            "\n",
            "Evaluating against bot 37: sunNervebot\n",
            "→ My agent vs sunNervebot: 6.2000\n",
            "\n",
            "Evaluating against bot 38: sweetrock\n",
            "→ My agent vs sweetrock: 24.2000\n",
            "\n",
            "Evaluating against bot 39: switchalot\n",
            "→ My agent vs switchalot: 26.2000\n",
            "\n",
            "Evaluating against bot 40: switchbot\n",
            "→ My agent vs switchbot: 53.9000\n",
            "\n",
            "Evaluating against bot 41: textbot\n",
            "→ My agent vs textbot: 139.6000\n",
            "\n",
            "Evaluating against bot 42: zq_move\n",
            "→ My agent vs zq_move: 30.2000\n",
            "Saved win rates to win_rates.json\n",
            "Saved win rates to win_rates.csv\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "win_rates = {}   # dictionary bot_name -> average return\n",
        "\n",
        "for bot_id, bot_name in enumerate(roshambo_bot_names):\n",
        "    print(f\"\\nEvaluating against bot {bot_id}: {bot_name}\")\n",
        "\n",
        "    my_agent = MyAgent(player_id=0, name=\"my_agent\", epsilon_action=0.8)\n",
        "    agents = [\n",
        "        my_agent,\n",
        "        create_roshambo_bot_agent(\n",
        "            1, num_actions, roshambo_bot_names, bot_id\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    avg_eval_returns = eval_agents(env, agents, num_players, 10, verbose=False)\n",
        "\n",
        "    # avg_eval_returns is an array like [my_return, opponent_return]\n",
        "    my_avg = float(avg_eval_returns[0])\n",
        "\n",
        "    print(f\"→ My agent vs {bot_name}: {my_avg:.4f}\")\n",
        "\n",
        "    win_rates[bot_name] = my_avg\n",
        "\n",
        "# ------------------------------\n",
        "# Save win rates to JSON\n",
        "# ------------------------------\n",
        "with open(\"win_rates.json\", \"w\") as f:\n",
        "    json.dump(win_rates, f, indent=2)\n",
        "print(\"Saved win rates to win_rates.json\")\n",
        "\n",
        "# ------------------------------\n",
        "# Save win rates to CSV\n",
        "# ------------------------------\n",
        "with open(\"win_rates.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"bot_name\", \"avg_return\"])\n",
        "    for bot_name, score in win_rates.items():\n",
        "        writer.writerow([bot_name, score])\n",
        "\n",
        "print(\"Saved win rates to win_rates.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.int64(40)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "something = pd.read_csv(\"win_rates.csv\")\n",
        "(something['avg_return'] >= 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "emg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
